permissions:
  contents: write

name: Fetch FlowScan builder‑fills archive

on:
  workflow_dispatch:    # allows manual runs
  schedule:
    - cron: '0 0 * * *'  # daily at 00:00 UTC

env:
  # one line per builder: BuilderName;BuilderAddress;StartDate(YYYY-MM-DD)
  BUILDERS: |
    pvp_dot_trade;0x0cbf655b0d22ae71fba3a674b0e1c0c7e7f975af;2024-10-27

jobs:
  fetch:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Purge stray root-level JSON files
        run: |
          rm -f data/*.json

      - name: Cleanup old error JSON files per builder
        run: |
          while IFS=';' read -r NAME ADDR START_DATE; do
            [ -z "$NAME" ] && continue
            mkdir -p data/$NAME
            FAIL_FILE="data/$NAME/failed_dates.json"
            [ ! -f "$FAIL_FILE" ] && echo '[]' > "$FAIL_FILE"

            for f in data/$NAME/*.json; do
              BASENAME=$(basename "$f")
              # skip meta files
              [[ "$BASENAME" =~ ^(failed_dates|daily_summary|user_data)(_.*)?\.json$ ]] && continue
              [ ! -f "$f" ] && continue

              DS=${BASENAME%.json}
              if jq -e 'has("error")' "$f" >/dev/null 2>&1; then
                echo "Deleting bad file $NAME/$DS.json"
                rm -f "$f"
                tmp=$(mktemp)
                jq --arg d "$DS" 'if index($d)==null then . + [$d] else . end' "$FAIL_FILE" > "$tmp" && mv "$tmp" "$FAIL_FILE"
              fi
            done
          done <<< "$BUILDERS"

      - name: Fetch daily JSONs, build user_data & daily_summary
        run: |
          DATE_YEST=$(date -d "yesterday" +%Y-%m-%d)

          while IFS=';' read -r NAME ADDR START_DATE; do
            [ -z "$NAME" ] && continue
            mkdir -p data/$NAME
            FAIL_FILE="data/$NAME/failed_dates.json"
            current="$START_DATE"

            # Fetch per-day JSONs
            while [ "$(date -d "$current" +%Y-%m-%d)" != "$(date -d "$DATE_YEST + 1 day" +%Y-%m-%d)" ]; do
              DS=$(date -d "$current" +%Y%m%d)
              OUTFILE="data/$NAME/$DS.json"
              RAW=$(curl -s "https://www.flowscan.xyz/api/builder-fills?builderAddress=$ADDR&date=$DS")

              if echo "$RAW" | jq 'has("error")' | grep -q true; then
                echo "Error for $NAME/$DS, logging"
                rm -f "$OUTFILE"
                tmp=$(mktemp)
                jq --arg d "$DS" 'if index($d)==null then . + [$d] else . end' "$FAIL_FILE" > "$tmp" && mv "$tmp" "$FAIL_FILE"
              else
                [ ! -f "$OUTFILE" ] && echo "$RAW" > "$OUTFILE"
                tmp=$(mktemp)
                jq --arg d "$DS" 'map(select(. != $d))' "$FAIL_FILE" > "$tmp" && mv "$tmp" "$FAIL_FILE"
              fi

              current=$(date -d "$current + 1 day" +%Y-%m-%d)
            done

            # Build daily_summary.json
            echo "Compiling daily_summary.json for $NAME"
            jq -n '
              [ inputs
                | { date: (input_filename | capture(".*/(?<d>[0-9]{8})\\.json$").d),
                    revenue: .revenue,
                    volume: .volume,
                    tradecount: .count,
                    uniqueusers: .uniqueUsers }
              ]
            ' data/$NAME/[0-9]*.json > data/$NAME/daily_summary.json

            # Build user_data.json (pre-aggregated per user)
            echo "Compiling user_data.json for $NAME"
            jq -s '
              [ .[] | (.userAggregations // [])[] ]
              | group_by(.user)
              | map({
                  user_address: .[0].user,
                  total_builder_fee_usd: (map(.builder_fee) | add),
                  total_volume: (map(.total_volume) | add),
                  total_trades: (map(.count) | add)
                })
            ' data/$NAME/[0-9]*.json > data/$NAME/user_data.json

            # Split user_data.json if >3MB
            SIZE=$(stat -c%s data/$NAME/user_data.json)
            if [ $SIZE -gt $((3*1024*1024)) ]; then
              echo "user_data.json too large; splitting into chunks"
              TOTAL=$(jq length data/$NAME/user_data.json)
              CHUNK=5000
              for ((i=0; i<$TOTAL; i+=CHUNK)); do
                j=$((i+CHUNK))
                jq ".[$i:$j]" data/$NAME/user_data.json > data/$NAME/user_data_${i}.json
              done
              rm data/$NAME/user_data.json
            fi

          done <<< "$BUILDERS"

      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data
          git diff --quiet --cached || (git commit -m "Update builder‑fills data for $(date -d \"yesterday\" +%Y%m%d)" && git push)
